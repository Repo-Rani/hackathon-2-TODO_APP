# Implementation Plan: Phase V - Event-Driven Advanced Features

**Branch**: `phase-5` | **Date**: 2026-02-15 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `specs/phase-5/spec.md`

## Summary

Phase V transforms the Todo App into an event-driven microservices architecture with Kafka and Dapr integration. The system introduces advanced features including recurring tasks, due dates with reminders, priorities, tags, search/filter, and real-time synchronization. All task operations publish events to Kafka for distributed processing, while Dapr provides state management, job scheduling, secrets management, and service invocation. The architecture is deployed on Minikube with local Kafka, ensuring scalability, resilience, and observability.

## Technical Context

**Language/Version**: Python 3.11+, TypeScript 5.0+
**Primary Dependencies**: FastAPI, SQLModel, PostgreSQL, Dapr 1.12+, Kafka 3.5+, Next.js 16
**Storage**: PostgreSQL (tasks, tags, reminders, activity logs) + Kafka (events) + Dapr State Store (conversation state)
**Testing**: pytest for backend, Jest/Vitest for frontend, integration tests for Kafka/Dapr
**Target Platform**: Minikube (Kubernetes) with local Kafka cluster
**Project Type**: Event-driven microservices with real-time web application
**Performance Goals**: 200ms p95 for task operations, 500ms p95 for search, 1s for real-time updates
**Constraints**: Local deployment (Minikube), no cloud services, backward compatibility with Phase I-IV
**Scale/Scope**: 1000+ concurrent users, 1000+ tasks per user, 100+ events per second

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### Spec-Driven Development Compliance
- ✓ All code will be generated by Claude Code from specifications
- ✓ Every feature has specification written first
- ✓ Will iterate on specs until Claude Code generates correct output
- ✓ Maintain `/specs` folder with all specification history

### Architecture Constraints Compliance
- ✓ Event-Driven Architecture: Kafka for event streaming, Dapr for distributed capabilities
- ✓ Minikube Deployment: Local Kubernetes with Kafka, Dapr sidecars, and microservices
- ✓ Backward Compatibility: Phase I-IV functionality preserved, only additions
- ✓ Database Evolution: Migrations add new tables and fields without breaking existing data
- ✓ Real-Time Synchronization: WebSocket/SSE for client updates backed by Kafka

### Code Quality Standards Compliance
- ✓ Python Style: Follow PEP 8
- ✓ Type Hints: Use type annotations for all functions
- ✓ Clean Code: Descriptive variable names, short functions
- ✓ Documentation: Docstrings for all public functions
- ✓ Error Handling: Proper HTTP status codes, Kafka error handling, Dapr retries

### Technology Stack Compliance
- ✓ Backend: Python 3.11+, FastAPI, SQLModel, PostgreSQL, Dapr SDK
- ✓ Frontend: Next.js 16 (App Router), TypeScript 5.0+, Tailwind CSS
- ✓ Event Streaming: Apache Kafka 3.5+ on Minikube
- ✓ Distributed Runtime: Dapr 1.12+ with Pub/Sub, State, Jobs, Secrets, Service Invocation
- ✓ Package Managers: UV (Python), npm/pnpm (JavaScript), Helm (Kubernetes)
- ✓ Development Tools: Claude Code, Spec-Kit Plus

### Data Model Principles Compliance
- ✓ Task Structure Extensions: Added due_datetime, priority, recurrence fields
- ✓ New Tables: tags, task_tags, reminders, activity_logs
- ✓ Relationships: Many-to-many for tasks-tags, foreign keys for reminders, activity logs
- ✓ Event Sourcing: All mutations logged to Kafka topics
- ✓ State Management: Conversation state in Dapr State Store
- ✓ Soft Delete: Activity logs track deletions for audit trail

### User Interface Principles Compliance
- ✓ Modern Web UI: Real-time updates via WebSocket/SSE
- ✓ Advanced Features: Recurring tasks, reminders, priorities, tags, search/filter
- ✓ Real-Time Synchronization: Changes appear across all connected clients
- ✓ Responsive Design: Works on mobile, tablet, desktop
- ✓ Accessibility: Follow WCAG guidelines for advanced UI components

### Event-Driven Architecture Compliance
- ✓ Kafka Integration: Local Kafka on Minikube, not Redpanda Cloud or Strimzi
- ✓ Event Schemas: Versioned JSON schemas for all event types
- ✓ Pub/Sub Pattern: Dapr Pub/Sub for event distribution
- ✓ Consumer Idempotency: All consumers handle duplicate messages gracefully
- ✓ Dead Letter Queue: Failed events routed to DLQ for analysis
- ✓ Partitioning Strategy: By user_id for ordering, by event_type for parallelism

### Dapr Integration Compliance
- ✓ Pub/Sub Component: Kafka for event streaming
- ✓ State Management Component: PostgreSQL for conversation state
- ✓ Jobs API Component: Scheduled reminders with persistence
- ✓ Secrets Management Component: Kubernetes secrets for sensitive data
- ✓ Service Invocation: Resilient service-to-service communication
- ✓ Observability: Distributed tracing, metrics, logging via Dapr

## Project Structure

### Documentation (this feature)

```text
specs/phase-5/
├── spec.md              # This file (/sp.spec command output)
├── plan.md              # This file (/sp.plan command output)
├── tasks.md             # /sp.tasks command output - NOT created by /sp.plan
├── event-schemas/       # Event schema definitions
│   ├── task-created.json
│   ├── task-updated.json
│   ├── task-completed.json
│   ├── task-deleted.json
│   └── reminder-triggered.json
├── dapr-components/      # Dapr component configurations
│   ├── kafka-pubsub.yaml
│   ├── conversation-state.yaml
│   ├── kubernetes-secrets.yaml
│   └── resiliency.yaml
└── diagrams/            # Event flow diagrams
    ├── architecture.png
    ├── event-flow.png
    └── kafka-topics.png
```

### Source Code (repository root)

```text
backend/
├── src/
│   ├── models/
│   │   ├── task.py              # Updated with new fields
│   │   ├── tag.py               # NEW: Tag model
│   │   ├── reminder.py          # NEW: Reminder model
│   │   └── activity_log.py      # NEW: ActivityLog model
│   ├── services/
│   │   ├── task_service.py      # Updated with advanced features
│   │   ├── recurring_task_service.py  # NEW: Recurring task logic
│   │   ├── reminder_service.py  # NEW: Reminder scheduling
│   │   ├── tag_service.py       # NEW: Tag management
│   │   ├── search_service.py    # NEW: Search and filter
│   │   ├── kafka_producer.py    # NEW: Kafka event publishing
│   │   ├── kafka_consumer.py    # NEW: Kafka event processing
│   │   └── activity_log_service.py  # NEW: Activity logging
│   ├── api/
│   │   ├── task_router.py       # Updated with new endpoints
│   │   ├── tag_router.py        # NEW: Tag endpoints
│   │   ├── reminder_router.py   # NEW: Reminder endpoints
│   │   ├── search_router.py     # NEW: Search endpoints
│   │   └── websocket_router.py  # NEW: WebSocket endpoint
│   ├── dapr/
│   │   ├── pubsub_client.py     # NEW: Dapr Pub/Sub wrapper
│   │   ├── state_client.py      # NEW: Dapr State Management wrapper
│   │   ├── jobs_client.py       # NEW: Dapr Jobs API wrapper
│   │   ├── secrets_client.py    # NEW: Dapr Secrets Management wrapper
│   │   └── invocation_client.py # NEW: Dapr Service Invocation wrapper
│   ├── events/
│   │   ├── event_schemas.py     # NEW: Event schema definitions
│   │   ├── serializers.py       # NEW: Event serialization
│   │   └── handlers.py          # NEW: Event handlers
│   ├── realtime/
│   │   ├── websocket_manager.py # NEW: WebSocket connection manager
│   │   └── sse_manager.py       # NEW: SSE connection manager
│   ├── migrations/
│   │   └── 005_add_advanced_features.py  # NEW: Database migration
│   └── main.py                  # Updated with Dapr sidecar
└── tests/
    ├── unit/
    ├── integration/
    └── performance/

frontend/
├── src/
│   ├── components/
│   │   ├── TaskForm.tsx          # Updated with advanced features
│   │   ├── TaskList.tsx          # Updated with search/filter/sort
│   │   ├── TaskItem.tsx          # Updated with priority, tags, due date
│   │   ├── RecurringTaskForm.tsx # NEW: Recurring task UI
│   │   ├── DueDatePicker.tsx     # NEW: Due date picker
│   │   ├── ReminderPicker.tsx   # NEW: Reminder time picker
│   │   ├── TagSelector.tsx       # NEW: Tag selection UI
│   │   ├── PriorityBadge.tsx     # NEW: Priority badge component
│   │   ├── SearchBar.tsx         # NEW: Search bar
│   │   ├── FilterPanel.tsx       # NEW: Filter panel
│   │   └── SortDropdown.tsx      # NEW: Sort dropdown
│   ├── pages/
│   │   ├── tasks.tsx             # Updated with advanced features
│   │   └── activity.tsx          # NEW: Activity feed page
│   ├── services/
│   │   ├── api.ts                # Updated with new endpoints
│   │   └── websocket.ts          # NEW: WebSocket client
│   ├── hooks/
│   │   ├── useRealtimeUpdates.ts # NEW: Real-time updates hook
│   │   └── useTaskFilters.ts     # NEW: Task filters hook
│   └── types/
│       └── task.ts               # Updated TypeScript types
└── tests/
    ├── unit/
    └── integration/

phase-5/
├── helm-charts/
│   └── todo-advanced/
│       ├── Chart.yaml
│       ├── values.yaml
│       └── templates/
│           ├── kafka/
│           │   ├── statefulset.yaml
│           │   └── service.yaml
│           ├── dapr/
│           │   ├── components/
│           │   │   ├── kafka-pubsub.yaml
│           │   │   ├── conversation-state.yaml
│           │   │   └── kubernetes-secrets.yaml
│           │   └── resiliency.yaml
│           └── backend/
│               ├── deployment.yaml  # Updated with Dapr sidecar
│               └── service.yaml
├── k8s/
│   ├── kafka/
│   │   ├── statefulset.yaml
│   │   └── service.yaml
│   ├── dapr/
│   │   ├── components/
│   │   │   ├── kafka-pubsub.yaml
│   │   │   ├── conversation-state.yaml
│   │   │   └── kubernetes-secrets.yaml
│   │   └── resiliency.yaml
│   └── backend/
│       ├── deployment.yaml        # Updated with Dapr sidecar
│       └── service.yaml
└── scripts/
    ├── setup-kafka.sh
    ├── setup-dapr.sh
    └── deploy-phase5.sh
```

**Structure Decision**: Extended the existing full-stack structure to support event-driven architecture with Kafka and Dapr. Backend services are organized by feature (tasks, reminders, tags) with dedicated Dapr clients for Pub/Sub, State, Jobs, Secrets, and Service Invocation. Frontend components are modularized to support advanced features with real-time synchronization via WebSocket.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| N/A | Constitution check passed | N/A |

## Scope and Dependencies

### In Scope

**Part A: Advanced Features (Local First)**
- Recurring tasks with daily/weekly/monthly patterns
- Due dates and time-based reminders
- Task priorities (High, Medium, Low)
- Tags/categories with many-to-many relationship
- Advanced search with keyword matching
- Multi-filter support (status, priority, due date, tags)
- Flexible sorting (due date, priority, creation date, alphabetical)
- Activity logging for audit trail

**Part B: Kafka & Dapr Integration (Minikube)**
- Local Kafka cluster on Minikube (not Redpanda Cloud or Strimzi)
- Dapr Pub/Sub component for Kafka integration
- Dapr State Management for conversation state persistence
- Dapr Jobs API for exact-time reminder scheduling (not cron bindings)
- Dapr Secrets Management for OPENAI_API_KEY, DATABASE_URL, KAFKA_CREDENTIALS
- Dapr Service Invocation for frontend-backend communication
- Real-time client synchronization via WebSocket/SSE
- Event-driven architecture with at-least-once delivery guarantees
- Dead letter queue for failed message processing

**Implementation**
- Database migrations for new tables and fields
- Backend services for recurring tasks, reminders, tags, search
- Kafka producers and consumers with event schemas
- Dapr component configuration (Pub/Sub, State, Jobs, Secrets)
- Frontend components for advanced features
- WebSocket connection manager for real-time updates
- Helm charts and Kubernetes manifests for deployment
- Testing (unit, integration, performance, security)

### Out of Scope

- Cloud-hosted Kafka (Redpanda Cloud) or Strimzi operator
- Cron-based reminder scheduling (use Dapr Jobs API instead)
- Email notifications for reminders (WebSocket/SSE only)
- Task templates or project management features
- Collaborative features (sharing tasks between users)
- Mobile app (web application only)
- Advanced analytics or reporting dashboards
- Time tracking or subtasks
- Attachments or file uploads

### External Dependencies

**Infrastructure**
- **Kafka**: Apache Kafka 3.5+ running on Minikube (local deployment)
- **Dapr**: Dapr 1.12+ runtime with sidecar injection
- **Kubernetes**: Minikube for local development and testing
- **PostgreSQL**: Existing Neon PostgreSQL database (Phase II)

**Libraries & SDKs**
- **dapr-sdk-python**: Python SDK for Dapr integration
- **confluent-kafka-python**: Python client for Kafka
- **fastapi-websocket-pubsub**: WebSocket support for FastAPI
- **sqlalchemy**: ORM for database operations (via SQLModel)
- **dateutil**: Date parsing and recurrence calculation

**Services**
- **No external cloud services** - All services run locally on Minikube
- **No third-party notification services** - Use WebSocket/SSE for notifications

### Dependency Ownership

- **Kafka Cluster**: DevOps team (setup and maintenance)
- **Dapr Runtime**: DevOps team (installation and configuration)
- **Backend Services**: Backend team (implementation and maintenance)
- **Frontend Application**: Frontend team (implementation and maintenance)
- **Database Migrations**: Backend team (SQL migrations)
- **Dapr Components**: DevOps team (YAML configuration)
- **Helm Charts**: DevOps team (packaging and deployment)

## Key Decisions and Rationale

### Decision 1: Local Kafka on Minikube (Not Redpanda Cloud or Strimzi)

**Options Considered**:
1. Redpanda Cloud (managed Kafka service)
2. Strimzi operator (Kubernetes-native Kafka)
3. Local Kafka on Minikube (standalone deployment)

**Trade-offs**:
- **Redpanda Cloud**: Easier setup, but requires cloud account, costs money, and violates "local deployment" requirement
- **Strimzi**: Kubernetes-native, but adds operator complexity and learning curve
- **Local Kafka on Minikube**: Requires manual setup, but fully local, no cloud dependencies, meets requirement

**Rationale**: Chose local Kafka on Minikube because the specification explicitly requires local deployment (Minikube, NOT cloud). Local Kafka provides full control, no external dependencies, and matches the project's development workflow. Setup is documented in scripts for reproducibility.

**Principles**: Smallest viable change (no cloud dependencies), reversible (can switch to cloud later), measurable (clear setup success criteria).

### Decision 2: Dapr Jobs API for Reminder Scheduling (Not Cron Bindings)

**Options Considered**:
1. Dapr Cron bindings
2. Dapr Jobs API (exact-time scheduling)
3. In-memory scheduling (Python schedulers)
4. Database polling

**Trade-offs**:
- **Cron bindings**: Simple, but only supports recurring schedules, not exact-time reminders
- **Jobs API**: More complex, but supports exact-time scheduling and survives restarts
- **In-memory**: Simple, but lost on restart, no persistence
- **Database polling**: Persistent, but inefficient and adds latency

**Rationale**: Chose Dapr Jobs API because reminders require exact-time scheduling (e.g., "remind me 30 minutes before due date"), not just recurring patterns. Jobs API provides persistence across restarts, which is critical for reliability. The specification explicitly mentions "exact-time reminders (NOT cron bindings)".

**Principles**: Measurable (reliability can be tested), reversible (can switch to cron if needed), smallest viable change (uses existing Dapr runtime).

### Decision 3: At-Least-Once Delivery for Kafka Events

**Options Considered**:
1. At-most-once (fire and forget)
2. At-least-once (guaranteed delivery, may have duplicates)
3. Exactly-once (strongest guarantee, higher complexity)

**Trade-offs**:
- **At-most-once**: Lowest latency, but risk of message loss
- **At-least-once**: Guaranteed delivery, requires idempotent consumers
- **Exactly-once**: Strongest guarantee, but higher complexity and latency

**Rationale**: Chose at-least-once because task mutations are critical and must not be lost. The complexity of exactly-once (transactional producers and consumers) is not justified for this use case. Idempotent consumers handle duplicates gracefully. Performance impact is acceptable (200ms p95 target).

**Principles**: Measurable (delivery rate can be monitored), reversible (can upgrade to exactly-once if needed), smallest viable change (standard Kafka configuration).

### Decision 4: Dapr State Management for Conversation State (Not Database)

**Options Considered**:
1. PostgreSQL database (existing tables)
2. Redis cache
3. Dapr State Management (PostgreSQL backend)
4. In-memory storage

**Trade-offs**:
- **PostgreSQL direct**: Familiar, but couples to database schema
- **Redis**: Fast, but adds another dependency
- **Dapr State Management**: Distributed, portable, supports multiple backends
- **In-memory**: Fastest, but lost on restart

**Rationale**: Chose Dapr State Management because it provides a distributed abstraction layer that works across multiple backend instances. Uses PostgreSQL as backend (no new infrastructure), but decouples the application from direct database access. Supports automatic retries and observability via Dapr.

**Principles**: Smallest viable change (uses existing PostgreSQL), reversible (can switch to direct DB access), measurable (latency and success rate monitored).

### Decision 5: WebSocket for Real-Time Updates (Not Polling)

**Options Considered**:
1. HTTP polling (periodic requests)
2. WebSocket (bidirectional persistent connection)
3. Server-Sent Events (SSE, unidirectional)
4. GraphQL subscriptions

**Trade-offs**:
- **HTTP polling**: Simple, but inefficient (unnecessary requests)
- **WebSocket**: Efficient, bidirectional, but more complex connection management
- **SSE**: Efficient, unidirectional, simpler than WebSocket
- **GraphQL subscriptions**: Powerful, but adds GraphQL dependency

**Rationale**: Chose WebSocket because it provides bidirectional communication (useful for future features like real-time chat) and is more efficient than polling. SSE was considered but WebSocket is more flexible. Connection complexity is manageable with proper error handling and reconnection logic.

**Principles**: Measurable (latency and connection stability monitored), smallest viable change (standard WebSocket API), reversible (can switch to SSE if needed).

### Decision 6: Many-to-Many Relationship for Tags (Not JSON Array)

**Options Considered**:
1. JSON array column in tasks table
2. Many-to-many relationship with separate tags table
3. Comma-separated string

**Trade-offs**:
- **JSON array**: Simple, but harder to query and enforce constraints
- **Many-to-many**: Normalized, enforceable constraints, queryable
- **Comma-separated**: Simple, but parsing required and not queryable

**Rationale**: Chose many-to-many relationship because tags are first-class entities with metadata (name, color). Normalized schema enforces data integrity (unique tags per user) and enables efficient queries (filter by tag). JSON array would make filtering and sorting difficult.

**Principles**: Smallest viable change (standard SQL pattern), measurable (query performance monitored), reversible (can migrate to JSON if needed).

### Decision 7: Event-Carried State Transfer (Not Event Sourcing)

**Options Considered**:
1. Event sourcing (store all events, rebuild state)
2. Event-carried state transfer (events carry full state)
3. Event notification (events only contain IDs)

**Trade-offs**:
- **Event sourcing**: Full audit trail, but complex rebuild logic
- **Event-carried state**: Consumers get full data, but larger events
- **Event notification**: Smallest events, but requires additional API calls

**Rationale**: Chose event-carried state transfer because consumers (activity logs, real-time sync) need the full task state immediately. Avoids additional API calls and simplifies consumer logic. Event events are not excessively large (<5KB per event). Event sourcing would add complexity without significant benefit.

**Principles**: Smallest viable change (simpler consumers), measurable (event size monitored), reversible (can switch to event sourcing if needed).

### Decision 8: Partition by User ID for Task Events

**Options Considered**:
1. Partition by user_id (ordering per user)
2. Partition by event_type (parallel processing)
3. Partition by task_id (ordering per task)
4. Round-robin (load balancing)

**Trade-offs**:
- **User_id**: Guarantees ordering per user, but may create hotspots
- **Event_type**: Parallel processing, but no ordering guarantees
- **Task_id**: Ordering per task, but consumers need all events
- **Round-robin**: Load balancing, but no ordering guarantees

**Rationale**: Chose partition by user_id because it guarantees ordering for a user's task events (critical for activity logs and real-time sync). Hotspots are mitigated by using 3 partitions (scales to thousands of users). Alternative would be to partition by user_id for task-events and by event_type for other topics.

**Principles**: Measurable (partition utilization monitored), reversible (can repartition if needed), smallest viable change (standard Kafka pattern).

## Interfaces and API Contracts

### Public APIs

#### Task API Extensions

**POST /api/{user_id}/tasks** - Create Task with Advanced Features

**Input**:
```json
{
  "title": "Complete project report",
  "description": "Final report for Q4",
  "due_datetime": "2026-02-20T15:00:00Z",
  "priority": "High",
  "recurrence_pattern": "weekly",
  "recurrence_end_date": "2026-06-30T23:59:59Z",
  "tag_ids": [1, 2, 3]
}
```

**Output (201 Created)**:
```json
{
  "id": 123,
  "user_id": "user-uuid",
  "title": "Complete project report",
  "description": "Final report for Q4",
  "completed": false,
  "due_datetime": "2026-02-20T15:00:00Z",
  "priority": "High",
  "recurrence_pattern": "weekly",
  "recurrence_end_date": "2026-06-30T23:59:59Z",
  "created_at": "2026-02-15T10:00:00Z",
  "updated_at": "2026-02-15T10:00:00Z",
  "tags": [
    {"id": 1, "name": "work", "color": "#3B82F6"},
    {"id": 2, "name": "urgent", "color": "#EF4444"}
  ],
  "reminder": null
}
```

**Errors**:
- 400 Bad Request: Invalid input (title missing, due_datetime in past)
- 401 Unauthorized: Missing or invalid authentication
- 403 Forbidden: User ID doesn't match authenticated user
- 422 Unprocessable Entity: Validation error (tag_ids don't exist)
- 500 Internal Server Error: Database or Kafka error

---

**PUT /api/{user_id}/tasks/{task_id}** - Update Task

**Input** (all fields optional):
```json
{
  "title": "Updated title",
  "due_datetime": "2026-02-21T15:00:00Z",
  "priority": "Medium",
  "recurrence_pattern": "none",
  "tag_ids": [1, 3]
}
```

**Output (200 OK)**: Same as task-created response

**Errors**: Same as create task

---

**GET /api/{user_id}/tasks** - List Tasks with Search, Filter, Sort

**Query Parameters**:
- `search` (optional): Keyword to search in title and description
- `status` (optional): "all" (default), "pending", "completed"
- `priority` (optional): "High", "Medium", "Low"
- `due_date_start` (optional): ISO 8601 datetime
- `due_date_end` (optional): ISO 8601 datetime
- `tags` (optional): Comma-separated tag IDs
- `sort_by` (optional): "created_at" (default), "due_datetime", "priority", "title"
- `sort_order` (optional): "desc" (default), "asc"
- `limit` (optional): Number of results (default: 50, max: 100)
- `offset` (optional): Pagination offset (default: 0)

**Example Request**:
```
GET /api/user-123/tasks?search=report&status=pending&priority=High&tags=1,2&sort_by=due_datetime&sort_order=asc&limit=20&offset=0
```

**Output (200 OK)**:
```json
{
  "tasks": [
    {
      "id": 123,
      "title": "Complete project report",
      "description": "Final report for Q4",
      "completed": false,
      "due_datetime": "2026-02-20T15:00:00Z",
      "priority": "High",
      "tags": [{"id": 1, "name": "work", "color": "#3B82F6"}],
      "created_at": "2026-02-15T10:00:00Z"
    }
  ],
  "total": 1,
  "limit": 20,
  "offset": 0
}
```

**Errors**: Same as create task

---

**POST /api/{user_id}/tasks/{task_id}/reminder** - Create Reminder

**Input**:
```json
{
  "reminder_time": "2026-02-20T14:30:00Z",
  "reminder_type": "due_date"
}
```

**Output (201 Created)**:
```json
{
  "id": 456,
  "task_id": 123,
  "reminder_time": "2026-02-20T14:30:00Z",
  "reminder_type": "due_date",
  "status": "pending",
  "created_at": "2026-02-15T10:00:00Z"
}
```

**Errors**:
- 400 Bad Request: reminder_time in past or invalid reminder_type
- 404 Not Found: Task doesn't exist or doesn't belong to user
- 422 Unprocessable Entity: Validation error
- 500 Internal Server Error: Dapr Jobs API or Kafka error

---

**DELETE /api/{user_id}/tasks/{task_id}/reminder** - Cancel Reminder

**Output (200 OK)**:
```json
{
  "id": 456,
  "status": "cancelled",
  "cancelled_at": "2026-02-15T11:00:00Z"
}
```

**Errors**:
- 404 Not Found: Reminder doesn't exist
- 500 Internal Server Error: Dapr Jobs API error

---

#### Tag API

**GET /api/{user_id}/tags** - List Tags

**Output (200 OK)**:
```json
{
  "tags": [
    {"id": 1, "name": "work", "color": "#3B82F6", "task_count": 5},
    {"id": 2, "name": "urgent", "color": "#EF4444", "task_count": 2}
  ]
}
```

**Errors**: Same as task API

---

**POST /api/{user_id}/tags** - Create Tag

**Input**:
```json
{
  "name": "personal",
  "color": "#10B981"
}
```

**Output (201 Created)**:
```json
{
  "id": 3,
  "user_id": "user-123",
  "name": "personal",
  "color": "#10B981",
  "created_at": "2026-02-15T10:00:00Z"
}
```

**Errors**:
- 400 Bad Request: Invalid name (empty or >50 chars)
- 409 Conflict: Tag with same name already exists for user
- 422 Unprocessable Entity: Validation error

---

**POST /api/{user_id}/tasks/{task_id}/tags** - Add Tag to Task

**Input**:
```json
{
  "tag_id": 3
}
```

**Output (201 Created)**: Updated task response

**Errors**:
- 404 Not Found: Task or tag doesn't exist
- 409 Conflict: Tag already added to task
- 422 Unprocessable Entity: Validation error

---

#### Activity Log API

**GET /api/{user_id}/activity-logs** - Get Activity Feed

**Query Parameters**:
- `limit` (optional): Number of results (default: 20, max: 100)
- `offset` (optional): Pagination offset (default: 0)

**Output (200 OK)**:
```json
{
  "activity_logs": [
    {
      "id": 789,
      "user_id": "user-123",
      "task_id": 123,
      "action": "task-completed",
      "old_values": {"completed": false},
      "new_values": {"completed": true},
      "created_at": "2026-02-15T10:00:00Z"
    },
    {
      "id": 788,
      "user_id": "user-123",
      "task_id": 123,
      "action": "task-created",
      "old_values": null,
      "new_values": {
        "title": "Complete project report",
        "priority": "High"
      },
      "created_at": "2026-02-15T09:00:00Z"
    }
  ],
  "total": 2,
  "limit": 20,
  "offset": 0
}
```

**Errors**: Same as task API

---

#### WebSocket API

**WS /api/ws/{user_id}** - Real-Time Updates

**Connection**: Establish WebSocket connection with user_id in path

**Client Message (Subscribe)**:
```json
{
  "action": "subscribe",
  "channels": ["task-updates", "reminders"]
}
```

**Server Message (Task Update)**:
```json
{
  "event_type": "task-updated",
  "data": {
    "task_id": 123,
    "user_id": "user-123",
    "changed_fields": ["completed"],
    "new_values": {"completed": true}
  },
  "timestamp": "2026-02-15T10:00:00Z"
}
```

**Server Message (Reminder)**:
```json
{
  "event_type": "reminder",
  "data": {
    "reminder_id": 456,
    "task_id": 123,
    "task_title": "Complete project report",
    "due_datetime": "2026-02-20T15:00:00Z"
  },
  "timestamp": "2026-02-15T14:30:00Z"
}
```

**Errors**:
- Connection rejected if user_id doesn't match authentication
- Connection closed on invalid message format

---

### Kafka Event Contracts

#### Topic: task-events

**Event Types**:
- `task-created`: New task created
- `task-updated`: Task modified
- `task-completed`: Task marked as complete
- `task-deleted`: Task deleted

**Event Schema (task-created)**:
```json
{
  "event_type": "task-created",
  "event_id": "uuid-v4",
  "timestamp": "2026-02-15T10:00:00Z",
  "version": "1.0",
  "data": {
    "task_id": 123,
    "user_id": "user-uuid",
    "title": "Complete project report",
    "description": "Final report for Q4",
    "completed": false,
    "due_datetime": "2026-02-20T15:00:00Z",
    "priority": "High",
    "recurrence_pattern": "weekly",
    "recurrence_end_date": "2026-06-30T23:59:59Z",
    "tags": ["work", "urgent"]
  }
}
```

**Event Schema (task-updated)**:
```json
{
  "event_type": "task-updated",
  "event_id": "uuid-v4",
  "timestamp": "2026-02-15T10:00:00Z",
  "version": "1.0",
  "data": {
    "task_id": 123,
    "user_id": "user-uuid",
    "changed_fields": ["completed", "priority"],
    "old_values": {
      "completed": false,
      "priority": "High"
    },
    "new_values": {
      "completed": true,
      "priority": "Medium"
    }
  }
}
```

---

#### Topic: reminders

**Event Types**:
- `reminder-triggered`: Reminder time arrived

**Event Schema**:
```json
{
  "event_type": "reminder-triggered",
  "event_id": "uuid-v4",
  "timestamp": "2026-02-15T10:00:00Z",
  "version": "1.0",
  "data": {
    "reminder_id": 456,
    "task_id": 123,
    "user_id": "user-uuid",
    "reminder_time": "2026-02-15T10:00:00Z",
    "task_title": "Complete project report",
    "task_due_datetime": "2026-02-20T15:00:00Z",
    "priority": "High"
  }
}
```

---

#### Topic: task-updates

**Event Types**:
- `task-created`, `task-updated`, `task-completed`, `task-deleted`

**Event Schema**: Same as task-events (for real-time client sync)

---

#### Topic: activity-logs

**Event Types**:
- `activity-log-created`: Activity log entry created

**Event Schema**:
```json
{
  "event_type": "activity-log-created",
  "event_id": "uuid-v4",
  "timestamp": "2026-02-15T10:00:00Z",
  "version": "1.0",
  "data": {
    "log_id": 789,
    "user_id": "user-uuid",
    "task_id": 123,
    "action": "task-completed",
    "old_values": {"completed": false},
    "new_values": {"completed": true}
  }
}
```

---

### Versioning Strategy

**API Versioning**:
- Current version: v1
- Version in URL path: `/api/v1/{user_id}/tasks`
- Backward compatibility maintained for Phase I-IV endpoints
- New endpoints only, no breaking changes to existing endpoints

**Event Schema Versioning**:
- Version field in each event: `"version": "1.0"`
- Semver: MAJOR.MINOR.PATCH
- MAJOR: Breaking changes (consumer must update)
- MINOR: Non-breaking additions (consumer can ignore)
- PATCH: Bug fixes (consumer unaffected)
- Consumers must validate version before processing

**Database Schema Versioning**:
- Migration files with version: `005_add_advanced_features.py`
- Sequential versioning (004, 005, 006...)
- Each migration is reversible (downgrade method)
- Version stored in `schema_migrations` table

---

### Idempotency, Timeouts, Retries

**API Idempotency**:
- POST /tasks: Not idempotent (use client-generated ID for idempotency)
- PUT /tasks/{task_id}: Idempotent (same result for repeated calls)
- DELETE /tasks/{task_id}: Idempotent (same result for repeated calls)
- DELETE /tasks/{task_id}/reminder: Idempotent

**Kafka Idempotency**:
- At-least-once delivery guarantees
- Consumers must handle duplicate events
- Use `event_id` (UUID) for deduplication
- Store processed event IDs in Redis or database

**Timeouts**:
- API calls: 30 seconds
- Kafka producer: 10 seconds
- Kafka consumer: 1 minute (max poll interval)
- Dapr State API: 5 seconds
- Dapr Jobs API: 10 seconds
- WebSocket: 60 seconds (ping interval)

**Retries**:
- API failures: Exponential backoff (1s, 2s, 4s, 8s, 16s)
- Kafka producer: 3 retries with exponential backoff
- Kafka consumer: Automatic (Kafka consumer group)
- Dapr APIs: 3 retries with exponential backoff
- Dapr resiliency policies configured globally

---

### Error Taxonomy with Status Codes

**4xx Client Errors**:
- 400 Bad Request: Invalid input, validation error
- 401 Unauthorized: Missing or invalid authentication
- 403 Forbidden: User ID mismatch, insufficient permissions
- 404 Not Found: Resource doesn't exist or doesn't belong to user
- 409 Conflict: Duplicate resource (tag name already exists)
- 413 Payload Too Large: Request body exceeds size limit
- 415 Unsupported Media Type: Invalid content type
- 422 Unprocessable Entity: Semantic validation error

**5xx Server Errors**:
- 500 Internal Server Error: Database, Kafka, or Dapr error
- 502 Bad Gateway: Dapr sidecar unavailable
- 503 Service Unavailable: Kafka broker down, rate limited
- 504 Gateway Timeout: Dapr API timeout

**Error Response Format**:
```json
{
  "error": {
    "code": "VALIDATION_ERROR",
    "message": "Title must be between 1 and 200 characters",
    "details": {
      "field": "title",
      "value": ""
    },
    "request_id": "req-uuid",
    "timestamp": "2026-02-15T10:00:00Z"
  }
}
```

---

## Non-Functional Requirements (NFRs) and Budgets

### Performance

**Latency Targets (p95)**:
- Task CRUD operations: 200ms
- Search and filter: 500ms
- Real-time updates: 1s (end-to-end)
- Reminder notification: 10s after scheduled time
- API response time: 300ms average

**Throughput Targets**:
- Task operations: 100 ops/second
- Kafka event publishing: 1000 events/second
- Kafka event consumption: 1000 events/second
- WebSocket messages: 500 messages/second

**Resource Caps**:
- Backend pod: 1000m CPU, 1Gi memory (as specified in Phase IV)
- Frontend pod: 500m CPU, 512Mi memory
- Kafka pod: 2000m CPU, 2Gi memory
- Dapr sidecar: 100m CPU, 128Mi memory per pod

**Performance Testing**:
- Load test: 1000 concurrent users, 10 tasks/second/user
- Stress test: 5000 concurrent users, 50 tasks/second
- Soak test: 24 hours at 50% capacity

---

### Reliability

**Service Level Objectives (SLOs)**:
- Availability: 99.9% (43 minutes downtime/month)
- Task operation success rate: 99%
- Kafka event delivery: 99.9%
- Reminder delivery: 99%
- WebSocket uptime: 99%

**Error Budgets**:
- Task operations: 1% error rate (10 errors per 1000 operations)
- Kafka events: 0.1% error rate (1 lost event per 1000)
- Reminders: 1% failure rate (10 missed reminders per 1000)

**Degradation Strategy**:
- Kafka down: Continue database operations, queue events, retry publishing
- Dapr down: Degrade to direct database access for critical operations
- WebSocket down: Fall back to HTTP polling (degraded UX)
- Database down: Return 503, show maintenance page

**Failure Modes**:
- Kafka broker failure: Retry with exponential backoff, queue events in memory
- Dapr sidecar failure: Restart pod, retry operations
- Database connection pool exhaustion: Return 503, scale backend pods
- High consumer lag: Scale consumer pods, increase partitions

---

### Security

**Authentication**:
- JWT tokens for all API endpoints
- WebSocket connections authenticated via token in query string
- Token expiration: 1 hour
- Refresh tokens: 24 hours

**Authorization**:
- User ID validation on all endpoints
- User data isolation enforced at API and database levels
- WebSocket connections scoped to user ID
- No cross-user data access allowed

**Data Handling**:
- Secrets stored in Dapr Secrets Management (Kubernetes secrets)
- No secrets in code or environment variables
- Database credentials via Dapr Secrets
- Kafka credentials via Dapr Secrets
- OPENAI_API_KEY via Dapr Secrets

**Encryption**:
- TLS for Kafka (production)
- HTTPS for all API calls
- WSS (WebSocket Secure) for WebSocket connections
- Database connections use SSL

**Auditing**:
- All task mutations logged to activity-logs table
- All events published to Kafka topics
- Audit trail includes user ID, timestamp, action, old/new values
- Log retention: 30 days

**Input Validation**:
- SQL injection prevention via parameterized queries
- XSS prevention via input sanitization
- CSRF protection via SameSite cookies
- Rate limiting: 100 requests/minute per user

---

### Cost

**Unit Economics (Minikube - Local)**:
- Infrastructure: $0 (local deployment)
- Development time: Estimated 40 hours for implementation
- Testing time: Estimated 20 hours for unit/integration/performance tests

**Cost Optimization**:
- Reuse existing PostgreSQL database (Phase II)
- Local Kafka (no cloud costs)
- Dapr sidecars share resources with pods
- Efficient Kafka partitioning (3 partitions, not 10+)

**Future Cloud Migration**:
- Kafka: Estimated $50/month for managed Kafka
- PostgreSQL: Already using Neon ($25/month)
- Dapr: No additional cost (runtime only)
- Kubernetes: $50-100/month for managed cluster
- Total: $125-175/month for cloud deployment

---

## Data Management and Migration

### Source of Truth

**Primary Data Store**:
- PostgreSQL database is the source of truth for tasks, tags, reminders, activity logs
- Kafka events are derived from database mutations
- Dapr State Store is source of truth for conversation state (AI chatbot)

**Consistency Model**:
- Strong consistency for database operations (ACID)
- Eventual consistency for Kafka events (at-least-once delivery)
- Strong consistency for Dapr State (linearizable)

---

### Schema Evolution

**New Tables**:
```sql
-- Tags table
CREATE TABLE tags (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    name VARCHAR(50) NOT NULL,
    color VARCHAR(7) DEFAULT '#3B82F6',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(user_id, name)
);

-- Task tags many-to-many table
CREATE TABLE task_tags (
    task_id INTEGER REFERENCES tasks(id) ON DELETE CASCADE,
    tag_id INTEGER REFERENCES tags(id) ON DELETE CASCADE,
    PRIMARY KEY (task_id, tag_id)
);

-- Reminders table
CREATE TABLE reminders (
    id SERIAL PRIMARY KEY,
    task_id INTEGER NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    reminder_time TIMESTAMP WITH TIME ZONE NOT NULL,
    reminder_type VARCHAR(20) NOT NULL DEFAULT 'due_date',
    status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'sent', 'cancelled')),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    sent_at TIMESTAMP WITH TIME ZONE
);

-- Activity logs table
CREATE TABLE activity_logs (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(255) NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    task_id INTEGER REFERENCES tasks(id) ON DELETE CASCADE,
    action VARCHAR(50) NOT NULL,
    old_values JSONB,
    new_values JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,
    INDEX(user_id, created_at),
    INDEX(task_id)
);
```

**New Columns in Tasks Table**:
```sql
ALTER TABLE tasks ADD COLUMN due_datetime TIMESTAMP WITH TIME ZONE;
ALTER TABLE tasks ADD COLUMN priority VARCHAR(10) DEFAULT 'Medium' CHECK (priority IN ('High', 'Medium', 'Low'));
ALTER TABLE tasks ADD COLUMN recurrence_pattern VARCHAR(20) CHECK (recurrence_pattern IN ('daily', 'weekly', 'monthly', 'none'));
ALTER TABLE tasks ADD COLUMN recurrence_end_date TIMESTAMP WITH TIME ZONE;
ALTER TABLE tasks ADD COLUMN parent_task_id INTEGER REFERENCES tasks(id);
```

---

### Migration and Rollback

**Migration File**: `backend/src/migrations/005_add_advanced_features.py`

**Migration Steps**:
1. Create new tables (tags, task_tags, reminders, activity_logs)
2. Add new columns to tasks table
3. Create indexes for performance
4. Populate default values for existing tasks
5. Validate migration

**Rollback Steps**:
1. Drop activity_logs table
2. Drop reminders table
3. Drop task_tags table
4. Drop tags table
5. Remove new columns from tasks table
6. Validate rollback

**Migration Testing**:
- Test on staging database before production
- Validate rollback plan works
- Measure migration time (<5 minutes)
- Test with existing data (1000+ tasks)

---

### Data Retention

**Database Retention**:
- Tasks: Retain indefinitely (user-managed deletion)
- Tags: Retain indefinitely (user-managed deletion)
- Reminders: Delete 30 days after sent or cancelled
- Activity logs: Retain 30 days, then archive or delete
- Deleted tasks: Soft delete for 7 days, then hard delete

**Kafka Retention**:
- task-events: 7 days
- reminders: 24 hours
- task-updates: 1 hour
- activity-logs: 30 days

**Dapr State Retention**:
- Conversation state: Retain 90 days
- Expire after 90 days of inactivity

---

## Operational Readiness

### Observability

**Logs**:
- Structured JSON logging
- Log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL
- Log fields: timestamp, level, service, user_id, request_id, message
- Log aggregation: Collect logs to file or ELK stack
- Log retention: 30 days

**Metrics**:
- Task operations: count, latency (p50, p95, p99)
- Kafka metrics: throughput, consumer lag, error rate
- Dapr metrics: API calls, latency, error rate
- WebSocket metrics: connections, messages, disconnections
- Custom metrics: recurrence instances created, reminders sent

**Traces**:
- Distributed tracing via Dapr (OpenTelemetry)
- Trace context: user_id, request_id, trace_id
- Span: API call, DB query, Kafka publish, Dapr call
- Trace sampling: 10% (adjustable)

**Dashboards**:
- Grafana dashboard for system health
- Metrics: requests/second, error rate, latency, consumer lag
- Alerts on: error rate > 1%, consumer lag > 1 min, uptime < 99%

---

### Alerting

**Alert Thresholds**:
- Error rate > 1% for 5 minutes
- Consumer lag > 1 minute for 5 minutes
- API latency p95 > 500ms for 5 minutes
- Kafka broker down
- Dapr sidecar not responding
- Database connection pool exhausted

**Alert Channels**:
- Email: ops-team@example.com
- Slack: #todo-app-alerts
- PagerDuty: On-call rotation (production only)

**Alert Escalation**:
- P1 (critical): Page on-call immediately
- P2 (high): Email + Slack within 5 minutes
- P3 (medium): Email within 30 minutes
- P4 (low): Daily digest

**On-Call Owners**:
- Backend team: Primary on-call for APIs, database
- DevOps team: Primary on-call for Kafka, Dapr, Kubernetes
- Frontend team: Secondary on-call for WebSocket, UI issues

---

### Runbooks for Common Tasks

**Runbook: Deploy New Version**
1. Build Docker images for backend and frontend
2. Push to container registry (local registry for Minikube)
3. Run database migrations
4. Update Helm chart version
5. Deploy via Helm: `helm upgrade todo-app ./helm-charts/todo-advanced`
6. Verify health checks pass
7. Monitor metrics and logs for 15 minutes
8. Rollback if issues detected

**Runbook: Handle Kafka Consumer Lag**
1. Check consumer lag via Kafka metrics
2. Identify slow consumer (logs, CPU, memory)
3. Scale consumer pods: `kubectl scale deployment backend --replicas=4`
4. If lag persists, increase Kafka partitions
5. Monitor consumer lag decreasing
6. Scale back when lag resolved

**Runbook: Restart Dapr Sidecar**
1. Identify pod with Dapr issues: `kubectl logs backend-pod-xyz -c daprd`
2. Restart pod: `kubectl delete pod backend-pod-xyz`
3. Verify Dapr sidecar starts: `kubectl logs backend-pod-xyz -c daprd`
4. Check Dapr health: `curl localhost:3500/v1.0/healthz`
5. Monitor metrics for recovery

**Runbook: Handle Reminder Job Failures**
1. Check Dapr Jobs API logs
2. Identify failed jobs: `kubectl logs backend-pod-xyz | grep "job-failed"`
3. Manually reschedule failed reminders
4. Check Dapr resiliency policies
5. Monitor job success rate

**Runbook: Database Migration Failure**
1. Check migration logs: `kubectl logs backend-pod-xyz | grep "migration"`
2. Identify error (schema conflict, data issue)
3. Fix migration script or rollback
4. Test migration on staging
5. Retry migration on production
6. Verify data integrity

---

### Deployment and Rollback

**Deployment Strategy**:
- Rolling update with Helm
- Max surge: 1 pod, max unavailable: 0
- Health checks before traffic routing
- Canary deployment (optional): 10% traffic first

**Deployment Steps**:
1. Run database migrations (first)
2. Build and push Docker images
3. Update Helm values (new image tag)
4. Deploy: `helm upgrade --install todo-app ./helm-charts/todo-advanced`
5. Verify: `kubectl rollout status deployment/todo-backend`
6. Smoke tests: Create task, verify Kafka event
7. Monitor: Check logs, metrics, alerts for 15 minutes

**Rollback Strategy**:
- Immediate rollback on critical errors
- Rollback via Helm: `helm rollback todo-app`
- Database rollback: Run downgrade migration
- Kafka events: Consumers handle version differences
- Monitor for recovery

**Feature Flags**:
- Enable/disable features via environment variables
- Flags: `ENABLE_RECURRING_TASKS`, `ENABLE_REMINDERS`, `ENABLE_REALTIME`
- Default: All features enabled
- Rollback: Disable specific feature if issues

---

### Feature Flags and Compatibility

**Feature Flags**:
```yaml
# In Helm values.yaml
featureFlags:
  recurringTasks: true
  reminders: true
  priorities: true
  tags: true
  search: true
  realtime: true
```

**Compatibility**:
- Backward compatible with Phase I-IV clients
- New fields optional in API requests
- New endpoints don't break existing endpoints
- Kafka events versioned for compatibility

**Deprecation Strategy**:
- Announce deprecation 3 months in advance
- Add deprecation header in API responses
- Maintain backward compatibility for 6 months
- Remove deprecated features after 9 months

---

## Risk Analysis and Mitigation

### Risk 1: Kafka Broker Failure

**Impact**: High (events not published, real-time sync broken)

**Blast Radius**: All event-driven features (activity logs, reminders, real-time sync)

**Mitigation**:
- Kafka replication (3 brokers) for high availability
- Dead letter queue for failed events
- Retry logic with exponential backoff (1s, 2s, 4s, 8s, 16s)
- Fallback to direct database operations for critical features
- Alert on Kafka broker down (P1)
- Runbook: Restart Kafka cluster

**Kill Switches/Guardrails**:
- Circuit breaker: Disable Kafka publishing if error rate > 10%
- Circuit breaker: Disable real-time sync if consumer lag > 5 minutes
- Monitor Kafka health: `/health` endpoint checks Kafka connection

---

### Risk 2: Dapr Sidecar Failure

**Impact**: Medium (state management, secrets, jobs unavailable)

**Blast Radius**: Dapr-dependent features (reminders, conversation state, secrets)

**Mitigation**:
- Dapr sidecar health checks and auto-restart
- Fallback to direct database access for critical operations
- Secrets cached locally (with TTL)
- Alert on Dapr sidecar down (P2)
- Runbook: Restart affected pods

**Kill Switches/Guardrails**:
- Feature flag: Disable reminders if Dapr Jobs API unavailable
- Feature flag: Disable real-time sync if Dapr Pub/Sub unavailable
- Monitor Dapr health: `/healthz` endpoint checks Dapr connection

---

### Risk 3: Reminder Job Failures

**Impact**: Medium (missed reminders, user frustration)

**Blast Radius**: Reminder system only

**Mitigation**:
- Dapr Jobs API persistence across restarts
- Retry failed jobs (3 attempts)
- Monitor job success rate (alert if < 99%)
- Manual reschedule for critical reminders
- Runbook: Check Dapr Jobs API logs

**Kill Switches/Guardrails**:
- Alert on job failure (P2)
- Disable reminder creation if failure rate > 5%
- Log all failed jobs for investigation

---

### Risk 4: Database Migration Failure

**Impact**: High (application broken, data loss risk)

**Blast Radius**: Entire application (all features depend on database)

**Mitigation**:
- Test migrations on staging before production
- Back up database before migration
- Reversible migrations (downgrade script)
- Migration rollback plan documented
- Runbook: Database migration failure

**Kill Switches/Guardrails**:
- Pause deployment if migration fails
- Rollback to previous version if migration不可逆
- Alert on migration failure (P1)

---

### Risk 5: WebSocket Connection Issues

**Impact**: Low (degraded UX, fallback to polling)

**Blast Radius**: Real-time sync only

**Mitigation**:
- Automatic reconnection with exponential backoff
- Event replay on reconnection (fetch missed events)
- Fallback to HTTP polling (30-second interval)
- Monitor connection count and error rate
- Runbook: WebSocket connection issues

**Kill Switches/Guardrails**:
- Disable WebSocket if error rate > 10%
- Enable fallback polling automatically
- Alert on WebSocket connection failures (P3)

---

### Risk 6: Consumer Lag

**Impact**: Medium (stale activity logs, delayed real-time updates)

**Blast Radius**: Event consumers (activity logs, real-time sync)

**Mitigation**:
- Monitor consumer lag (alert if > 1 minute)
- Scale consumer pods dynamically based on lag
- Increase Kafka partitions if needed
- Optimize consumer performance (batch processing)
- Runbook: Handle consumer lag

**Kill Switches/Guardrails**:
- Scale consumers automatically when lag > 30 seconds
- Pause event publishing if lag > 5 minutes
- Alert on consumer lag (P2)

---

## Evaluation and Validation

### Definition of Done

**Unit Tests**:
- All new service methods have unit tests
- Event serialization/deserialization tested
- Recurrence date calculation logic tested
- Search and filter query builders tested
- Test coverage: >80%

**Integration Tests**:
- End-to-end task operations with Kafka event publishing
- Recurring task completion and instance creation
- Reminder scheduling and triggering
- Real-time WebSocket updates
- Dapr component integration (Pub/Sub, State, Jobs, Secrets)
- Database migrations and rollback

**Performance Tests**:
- Load test: 1000 concurrent users
- Stress test: 5000 concurrent users
- Soak test: 24 hours at 50% capacity
- Performance targets met (200ms p95 for task operations)

**Security Tests**:
- Secret leakage verification (no secrets in logs)
- Cross-user data isolation enforcement
- SQL injection prevention
- WebSocket authentication and authorization
- Dependency vulnerability scan

**Code Quality**:
- All code reviewed and approved
- Follows PEP 8 and project conventions
- Documentation complete (docstrings, comments)
- No TODOs or FIXMEs in production code

**Documentation**:
- API documentation updated (Swagger/OpenAPI)
- Event schemas documented in `/specs/phase-5/event-schemas/`
- Dapr component configurations documented
- Runbooks created and tested
- Deployment guide updated

---

### Output Validation

**Format Validation**:
- JSON schemas validated for all API responses
- Event schemas validated for all Kafka events
- TypeScript types match API contracts
- Database schema matches migration scripts

**Requirements Validation**:
- All user stories tested and verified
- All functional requirements (FR-101 to FR-160) met
- All non-functional requirements (NFR-101 to NFR-130) met
- Success criteria (SC-001 to SC-012) achieved

**Safety Validation**:
- No security vulnerabilities (OWASP Top 10)
- No data leaks or cross-user access
- SQL injection prevention verified
- XSS and CSRF prevention verified
- Secrets properly managed (Dapr Secrets)

**Compatibility Validation**:
- Phase I-IV functionality preserved
- Backward compatibility tested
- Database migrations reversible
- Kafka event versioning works
- WebSocket reconnection and event replay tested

---

## Architectural Decision Record (ADR)

**Significant Decisions**:
1. Local Kafka on Minikube (Not Redpanda Cloud or Strimzi)
2. Dapr Jobs API for Reminder Scheduling (Not Cron Bindings)
3. At-Least-Once Delivery for Kafka Events
4. Dapr State Management for Conversation State (Not Database)
5. WebSocket for Real-Time Updates (Not Polling)
6. Many-to-Many Relationship for Tags (Not JSON Array)
7. Event-Carried State Transfer (Not Event Sourcing)
8. Partition by User ID for Task Events

**ADR Creation**:
Each significant decision above should be documented as an ADR in `/history/adr/`. Suggest creating ADRs for:
- ADR-001: Local Kafka on Minikube for Phase V
- ADR-002: Dapr Jobs API for Reminder Scheduling
- ADR-003: At-Least-Once Delivery with Idempotent Consumers
- ADR-004: Dapr State Management for Conversation State
- ADR-005: WebSocket vs SSE for Real-Time Updates

**ADR Template**: Use `/history/adr/ADR-template.md` (create if doesn't exist)

**ADR Review**: Review all ADRs with team before implementation
